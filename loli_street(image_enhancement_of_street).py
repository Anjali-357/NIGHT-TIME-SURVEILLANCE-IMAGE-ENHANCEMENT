# -*- coding: utf-8 -*-
"""LoLI-Street(Image Enhancement of Street).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Ur6akK3mNZQWz6YsoEc1xGRv8hEiUhhA
"""

import os
from google.colab import drive

# --- 1. Mount Google Drive ---
print("--- Attempting to mount Google Drive ---")
try:
    drive.mount('/content/drive')
    print("\nâœ… Google Drive Mounted successfully at /content/drive")
except Exception as e:
    print(f"\nâŒ Error mounting Drive: {e}")
    print("Please check your authorization or try running the cell again.")


# --- 2. Verify Dataset Path ---
print("\n--- Verifying Dataset Location ---")
base_path = '/content/drive/MyDrive'
dataset_folder = 'LoLI_Street_Dataset_extracted'
full_path = os.path.join(base_path, dataset_folder)

if os.path.exists(full_path):
    print(f"\nFound dataset folder at: {full_path}")
    print("\nListing first 5 items in the folder to verify content:")
    # List a few files to ensure you're in the right directory
    print(os.listdir(full_path)[:5])
else:
    print(f"\nâŒ Dataset folder NOT FOUND at expected location: {full_path}")

import os

# The base path is the folder you found that contains the 'LoLI-Street Dataset' folder
BASE_PATH = '/content/drive/MyDrive/LoLI_Street_Dataset_extracted'
DATASET_ROOT = os.path.join(BASE_PATH, 'LoLI-Street Dataset')
MAX_DEPTH = 4 # Limit the display to 4 levels deep for clarity

def display_dataset_structure(root_dir, max_depth=3):
    """
    Traverses the directory structure and prints it in a tree format.
    """
    print(f"\n--- Exploring Dataset Structure: {root_dir} ---")

    if not os.path.isdir(root_dir):
        print(f"âŒ Error: Root directory not found or is not a directory: {root_dir}")
        return

    # os.walk yields (dirpath, dirnames, filenames)
    for root, dirs, files in os.walk(root_dir):
        # Calculate the current depth relative to the starting point
        depth = root[len(root_dir):].count(os.sep)

        if depth > max_depth:
            # Skip deeper levels
            del dirs[:] # Prevent descending into deeper directories
            continue

        # Create indentation for tree structure
        indent = '    ' * depth

        # Determine the name of the current directory
        dir_name = os.path.basename(root) if root != root_dir else os.path.basename(root_dir)

        # Print the directory name
        print(f"{indent}ðŸ“‚ {dir_name}/")

        # Indent files one level deeper
        file_indent = indent + '    '

        # Print file list or summary
        if files:
            file_count = len(files)

            # Print up to 3 file names, then summarize the rest
            if file_count > 3:
                print(f"{file_indent}â”œâ”€ ðŸ“„ {files[0]}")
                print(f"{file_indent}â”œâ”€ ðŸ“„ {files[1]}")
                print(f"{file_indent}â”œâ”€ ðŸ“„ {files[2]}")
                print(f"{file_indent}â””â”€ ({file_count - 3} more files...)")
            else:
                for file_name in files:
                    print(f"{file_indent}â””â”€ ðŸ“„ {file_name}")

        # Don't descend into hidden/system directories
        dirs[:] = [d for d in dirs if not d.startswith('.')]

    print("\n--- Structure Exploration Complete ---")
    print("\nBased on this structure, you can determine the correct path for your low-light images.")

# Execute the function
display_dataset_structure(DATASET_ROOT, max_depth=MAX_DEPTH)

import cv2
import numpy as np
import matplotlib.pyplot as plt
import os
from google.colab import files
from IPython.display import display, Image

# --- 1. Core Enhancement Pipeline Functions (V2: Non-Local Means and Saturation Boost) ---

def multi_stage_enhancement(bgr_img):
    """
    Applies 10 sequential advanced image processing techniques for high-clarity
    night surveillance enhancement.
    """
    if bgr_img is None:
        return {}

    # Dictionary to store all steps for visualization
    results = {'Step 1: Original Image (BGR)': bgr_img}
    current_img = bgr_img.copy()

    # --- Step 2: ADVANCED DENOISING (Non-Local Means) ---
    # This is a superior method for preserving edges while removing noise.
    current_img = cv2.fastNlMeansDenoisingColored(current_img, None, 10, 10, 7, 21)
    results['Step 2: Non-Local Means Denoising (Color)'] = current_img.copy()

    # --- Step 3: Color Model Conversion to HSV ---
    hsv_img = cv2.cvtColor(current_img, cv2.COLOR_BGR2HSV)
    h, s, v = cv2.split(hsv_img)
    results['Step 3: HSV V-Channel Isolation (Brightness)'] = cv2.cvtColor(v, cv2.COLOR_GRAY2BGR)

    # --- Step 4: Local Contrast Enhancement (Optimized CLAHE on V) ---
    # Optimized settings to aggressively brighten shadows without washing out highlights.
    clahe = cv2.createCLAHE(clipLimit=3.5, tileGridSize=(10, 10))
    v_enhanced = clahe.apply(v)
    results['Step 4: CLAHE on V-Channel (Local Brightness)'] = cv2.cvtColor(v_enhanced, cv2.COLOR_GRAY2BGR)

    # --- Step 5: Color Saturation Boost (on S-channel) ---
    # Boost saturation to recover richness lost in dark images.
    s_boosted = cv2.addWeighted(s, 1.25, s, 0, 0) # Increase saturation by 25%
    s_boosted = np.clip(s_boosted, 0, 255).astype(np.uint8)
    results['Step 5: Saturation Boost (S-Channel)'] = cv2.cvtColor(s_boosted, cv2.COLOR_GRAY2BGR)

    # --- Step 6: Merge Channels and Convert Back to BGR ---
    current_img = cv2.merge([h, s_boosted, v_enhanced])
    current_img = cv2.cvtColor(current_img, cv2.COLOR_HSV2BGR)
    results['Step 6: Enhanced Color & Contrast Image'] = current_img.copy()

    # --- Step 7: Unsharp Masking (Sharpening) ---
    # Sharpening applied to the now clean and high-contrast image.
    blurred = cv2.GaussianBlur(current_img, (0, 0), 3.0)
    sharpened_bgr = cv2.addWeighted(current_img, 1.5, blurred, -0.5, 0)
    current_img = sharpened_bgr
    results['Step 7: Final Sharpened Color Image (V2 Enhancement)'] = current_img.copy()

    # --- Feature Extraction Steps (Applied to final enhanced image) ---
    gray_sharpened = cv2.cvtColor(current_img, cv2.COLOR_BGR2GRAY)

    # --- Step 8: Canny Edge Detection (Feature Map 1) ---
    canny_edges = cv2.Canny(gray_sharpened, threshold1=80, threshold2=220)
    results['Step 8: Canny Edge Map (High Contrast)'] = cv2.cvtColor(canny_edges, cv2.COLOR_GRAY2BGR)

    # --- Step 9: Morphological Dilation (Thicken Edges) ---
    kernel = np.ones((2,2), np.uint8)
    dilated_edges = cv2.dilate(canny_edges, kernel, iterations=1)
    results['Step 9: Morphological Dilation (Thickened Edges)'] = cv2.cvtColor(dilated_edges, cv2.COLOR_GRAY2BGR)

    # --- Step 10: Harris Corner Detection (Feature Map 2) ---
    harris_dst = cv2.cornerHarris(gray_sharpened, blockSize=2, ksize=3, k=0.04)
    harris_dst = cv2.dilate(harris_dst, None)
    corner_map = sharpened_bgr.copy()
    corner_map[harris_dst > 0.01 * harris_dst.max()] = [0, 0, 255] # Mark corners in Red
    results['Step 10: Harris Corner Map (Tracking Features)'] = corner_map

    # Final Result for comparison
    results['Final Enhanced Image (Best Result)'] = sharpened_bgr

    return results

# --- 2. Visualization Function (Updated to use 3x4 grid structure) ---

def visualize_results(results_dict, image_file_name):
    """Plots the 12 resulting images in a 3x4 grid with highlighting."""
    if not results_dict:
        return

    titles = list(results_dict.keys())
    images = list(results_dict.values())

    # Select the first 12 results (1 Original + 9 Steps + 2 Final)
    titles_to_plot = titles[:12]
    images_to_plot = images[:12]

    fig, axes = plt.subplots(3, 4, figsize=(22, 16))
    fig.suptitle(f"Advanced 10-Step Enhancement Pipeline V2: {image_file_name}", fontsize=20, weight='bold')

    # Index 0 is Original. Index 6 is the final enhanced image (Step 7).
    HIGHLIGHT_INDICES = [0, 6]

    for i, ax in enumerate(axes.flat):
        if i < len(images_to_plot):
            rgb_img = cv2.cvtColor(images_to_plot[i], cv2.COLOR_BGR2RGB)

            is_highlighted = i in HIGHLIGHT_INDICES

            ax.imshow(rgb_img)
            ax.set_title(titles_to_plot[i], fontsize=11,
                         color='red' if is_highlighted else 'black',
                         weight='bold' if is_highlighted else 'normal')

            # Add a visual border for highlighting
            if is_highlighted:
                 for spine in ax.spines.values():
                     spine.set_color('red')
                     spine.set_linewidth(3)

            ax.axis('off')
        else:
            ax.axis('off')

    plt.tight_layout(rect=[0, 0.03, 1, 0.97])
    plt.show()

# --- 3. Interactive Execution Block ---

def handle_upload_and_process():
    """Prompts user for image upload and initiates processing."""
    print("\n--- Advanced Image Enhancement Interactive Processor ---")
    print("Please upload a single low-light or dark image from your local computer for high-clarity processing.")

    uploaded = files.upload()

    if not uploaded:
        print("No file uploaded. Processing cancelled.")
        return

    filename = list(uploaded.keys())[0]
    image_bytes = uploaded[filename]

    try:
        nparr = np.frombuffer(image_bytes, np.uint8)
        bgr_img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

        if bgr_img is None:
            raise ValueError("Could not decode the image file.")

        print(f"\nProcessing image: {filename}...")

        # --- Run the 10-Step Pipeline ---
        all_steps = multi_stage_enhancement(bgr_img)

        # --- Visualize Results ---
        visualize_results(all_steps, filename)

        print(f"\nâœ… V2 Pipeline executed successfully on {filename}.")
        print("The **Original Image** and the **Final Sharpened Color Image (Step 7)** are highlighted in red for comparison.")

    except Exception as e:
        print(f"\nâŒ An error occurred during processing: {e}")
        # Print a helpful hint for common Colab issues
        if 'decode' in str(e):
             print("\nHint: Ensure the uploaded file is a standard JPEG or PNG file.")

# Execute the interactive process
handle_upload_and_process()

